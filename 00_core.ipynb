{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc_collection\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nbdev.showdoc import *\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import json\n",
    "import pkg_resources\n",
    "import pkgutil\n",
    "import pandas\n",
    "import warnings\n",
    "import pydoc\n",
    "from pathlib import Path\n",
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace(list_, str1, str2):\n",
    "    i = list_.index(str1)\n",
    "    return list_[:i] + [str2] + list_[i + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_top_hundred():\n",
    "    if not os.path.isfile('data/top.json'):\n",
    "        os.system('curl -X GET \"https://hugovk.github.io/top-pypi-packages/top-pypi-packages-30-days.json\" > data/top.json')\n",
    "\n",
    "    f = open('data/top.json')\n",
    "    data = json.load(f)\n",
    "\n",
    "    return [dictionary['project'] for dictionary in data['rows'][:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def pip_top_hundred():\n",
    "    lib_names = get_top_hundred()\n",
    "    lib_names = replace(lib_names, 'msrest', 'msrestazure')\n",
    "    for lib_name in lib_names:\n",
    "        os.system('pip install ' + lib_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def val(key):\n",
    "    return key[0] != '_' and key[-1] != '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extr(module_name):\n",
    "    names = []\n",
    "    texts = []\n",
    "    paths = []\n",
    "\n",
    "    finded_names, finded_texts, finded_paths = recclass(module_name, module_name.__name__)\n",
    "    names.extend(finded_names)\n",
    "    texts.extend(finded_texts)\n",
    "    paths.extend(finded_paths)\n",
    "\n",
    "    if hasattr(module_name, '__path__'):\n",
    "        file = module_name.__path__\n",
    "    else:\n",
    "        file = [inspect.getfile(module_name)]\n",
    "\n",
    "    for importer, key, ispkg in pkgutil.iter_modules(file):\n",
    "        if val(key) and ispkg and hasattr(module_name, key):\n",
    "            if key != module_name.__name__ and fullname(getattr(module_name, key)).startswith(fullname(module_name)):\n",
    "                finded_names, finded_texts, finded_paths = extr(getattr(module_name, key))\n",
    "                names.extend(finded_names)\n",
    "                texts.extend(finded_texts)\n",
    "                paths.extend(finded_paths)\n",
    "\n",
    "    return names, texts, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def recclass(cl, path):\n",
    "    names = []\n",
    "    texts = []\n",
    "    paths = []\n",
    "\n",
    "    names.append(cl)\n",
    "    texts.append(pydoc.render_doc(cl, renderer=pydoc.TextDoc()))\n",
    "    paths.append(path)\n",
    "\n",
    "    for key_, value_ in inspect.getmembers(cl, inspect.isroutine):\n",
    "        if val(key_) and hasattr(cl, key_):\n",
    "            names.append(getattr(cl, key_))\n",
    "            texts.append(pydoc.render_doc(getattr(cl, key_), renderer=pydoc.TextDoc()))\n",
    "            paths.append(path + '.' + key_)\n",
    "\n",
    "    for key_, value_ in inspect.getmembers(cl, inspect.isfunction):\n",
    "        if val(key_) and hasattr(cl, key_):\n",
    "            names.append(getattr(cl, key_))\n",
    "            texts.append(pydoc.render_doc(getattr(cl, key_), renderer=pydoc.TextDoc()))\n",
    "            paths.append(path + '.' + key_)\n",
    "\n",
    "    for key_, value_ in inspect.getmembers(cl, inspect.isdatadescriptor):\n",
    "        if val(key_) and hasattr(cl, key_):\n",
    "            names.append(getattr(cl, key_))\n",
    "            texts.append(pydoc.render_doc(getattr(cl, key_), renderer=pydoc.TextDoc()))\n",
    "            paths.append(path + '.' + key_)\n",
    "\n",
    "    for key_, value_ in inspect.getmembers(cl, inspect.isclass):\n",
    "        if val(key_) and hasattr(cl, key_):\n",
    "            if fullname(getattr(cl, key_)).startswith(fullname(cl)):\n",
    "                finded_names, finded_texts, finded_paths = recclass(getattr(cl, key_), path + '.' + key_)\n",
    "                names.extend(finded_names)\n",
    "                texts.extend(finded_texts)\n",
    "                paths.extend(finded_paths)\n",
    "\n",
    "    return names, texts, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fullname(o):\n",
    "\n",
    "    if inspect.ismodule(o):\n",
    "        return o.__name__\n",
    "\n",
    "    if inspect.isfunction(o) or inspect.isclass(o) or inspect.ismethod(o):\n",
    "        return o.__module__ + '.' + o.__name__\n",
    "\n",
    "    if inspect.isroutine(o):\n",
    "        if hasattr(o, '__name__'):\n",
    "            return o.__class__.__module__ + '.' + o.__name__\n",
    "        else:\n",
    "            return o.__class__.__module__ + '.' + o.__class__.__name__\n",
    "\n",
    "    if inspect.isdatadescriptor(o):\n",
    "        if o.__class__.__name__ == 'property' and o.fget != None:\n",
    "            if o.fget.__module__ != None:\n",
    "                return o.fget.__module__ + '.' + o.fget.__name__\n",
    "            else:\n",
    "                return o.fget.__name__\n",
    "        else:\n",
    "            return o.__class__.__module__ + '.' + o.__class__.__name__\n",
    "\n",
    "    return o.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "    installed_packages = pkg_resources.working_set\n",
    "    installed_packages_list = sorted([\"%s\" % i.key for i in installed_packages])\n",
    "\n",
    "    if installed_packages_list.count('transformers'):\n",
    "        installed_packages_list.remove('transformers')\n",
    "\n",
    "    unsolved_package_names = ['beautifulsoup4', 'typing-extensions', 'pyyaml', 'argon2-cffi', 'jupyter-client', 'jupyter-core']\n",
    "    unsolved_import_names = ['bs4', 'typing', 'yaml', 'argon2', 'jupyter_client', 'jupyter_core']\n",
    "\n",
    "    for i in range(len(unsolved_package_names)): #libs which I cant do general way\n",
    "        if installed_packages_list.count(unsolved_package_names[i]):\n",
    "            installed_packages_list = replace(installed_packages_list, unsolved_package_names[i], unsolved_import_names[i])\n",
    "\n",
    "    names = []\n",
    "    texts = []\n",
    "    paths = []\n",
    "\n",
    "    c = 0\n",
    "    for lib_name in installed_packages_list:\n",
    "        try:\n",
    "            importlib.import_module(lib_name)\n",
    "\n",
    "            finded_names, finded_texts, finded_paths = extr(sys.modules[lib_name])\n",
    "            names.extend(finded_names)\n",
    "            texts.extend(finded_texts)\n",
    "            paths.extend(finded_paths)\n",
    "\n",
    "            print(len(names) - c, 'files from', lib_name, 'added')\n",
    "            c = len(names)\n",
    "\n",
    "        except Exception:\n",
    "            try:\n",
    "                metadata_dir = pkg_resources.get_distribution(lib_name).egg_info\n",
    "                for name in open('%s/%s' % (metadata_dir, 'top_level.txt')).read().rstrip().split('\\n'):\n",
    "                    if name != '' and val(name):\n",
    "                        importlib.import_module(name)\n",
    "\n",
    "                        finded_names, finded_texts, finded_paths = extr(sys.modules[name])\n",
    "                        names.extend(finded_names)\n",
    "                        texts.extend(finded_texts)\n",
    "                        paths.extend(finded_paths)\n",
    "\n",
    "                        print(len(names) - c, 'objects from', name, 'was found and added to dataframe')\n",
    "                        c = len(names)\n",
    "\n",
    "            except Exception:\n",
    "                print('--------------- error during ' + lib_name + ' documentation extracting')\n",
    "\n",
    "    doc = pandas.DataFrame({'text': texts, 'path': paths})\n",
    "    doc_wo_dupl = doc.drop_duplicates(subset=['text'])\n",
    "    d = doc_wo_dupl.groupby('text').agg(paths = ('path', lambda x: list(x)), library = ('path', lambda x: list(x)[0].split('.')[0])).reset_index()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_batch(docs):\n",
    "    \"\"\" Indexes a batch of documents.\"\"\"\n",
    "    requests = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        request = doc\n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = 'doc'\n",
    "        request[\"text\"] = doc['text']\n",
    "        request[\"paths\"] = doc['paths']\n",
    "        request[\"library\"] = doc['library']\n",
    "        requests.append(request)\n",
    "    helpers.bulk(es, requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def es_add_bulk(d):\n",
    "    es = Elasticsearch('http://localhost:9200')\n",
    "\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 2,\n",
    "            \"number_of_replicas\": 1\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"dynamic\": \"true\",\n",
    "            \"_source\": {\n",
    "            \"enabled\": \"true\"\n",
    "            },\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"paths\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"library\": {\n",
    "                    \"type\": \"text\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if es.indices.exists(index=\"doc\"):\n",
    "        es.indices.delete(index='doc')\n",
    "\n",
    "    es.indices.create(index='doc', body=settings)\n",
    "    \n",
    "    index_batch([d[i] for i in d.index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
